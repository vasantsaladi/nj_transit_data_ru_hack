{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing data...\n",
      "Preparing features...\n",
      "Using features: ['hour_of_day', 'day_of_week', 'month', 'is_weekend', 'is_rush_hour', 'is_peak_month', 'from_id', 'to_id', 'route_frequency', 'route_avg_delay', 'line_Atl. City Line', 'line_Bergen Co. Line ', 'line_Gladstone Branch', 'line_Main Line', 'line_Montclair-Boonton', 'line_Morristown Line', 'line_No Jersey Coast', 'line_Northeast Corrdr', 'line_Pascack Valley', 'line_Princeton Shuttle', 'line_Raritan Valley', 'type_NJ Transit', 'status_cancelled', 'status_departed', 'status_estimated']\n",
      "Splitting data...\n",
      "Training model...\n",
      "Evaluating model...\n",
      "\n",
      "Model Performance:\n",
      "Mean Absolute Error (MAE): 1.36 minutes\n",
      "Root Mean Squared Error (RMSE): 1.83 minutes\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "             feature  importance\n",
      "9    route_avg_delay    0.485612\n",
      "0        hour_of_day    0.140571\n",
      "2              month    0.068047\n",
      "8    route_frequency    0.058748\n",
      "1        day_of_week    0.028371\n",
      "23   status_departed    0.027984\n",
      "4       is_rush_hour    0.025491\n",
      "22  status_cancelled    0.020280\n",
      "7              to_id    0.019196\n",
      "6            from_id    0.018920\n",
      "\n",
      "Testing prediction...\n",
      "Predicted delay for Newark Penn Station to Newark Penn Station: 0.22 minutes\n",
      "\n",
      "Process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor  # Using this instead of XGBoost\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Step 1: Load Data\n",
    "print(\"Loading data...\")\n",
    "data_path = '/Users/chetan/Documents/GitHub/nj_transit_data_ru_hack/data/data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Step 2: Enhanced Data Preprocessing\n",
    "print(\"Preprocessing data...\")\n",
    "\n",
    "# Convert datetime columns\n",
    "df['scheduled_time'] = pd.to_datetime(df['scheduled_time'], errors='coerce')\n",
    "df['actual_time'] = pd.to_datetime(df['actual_time'], errors='coerce')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Drop rows where time conversion failed\n",
    "df = df.dropna(subset=['scheduled_time', 'actual_time'])\n",
    "\n",
    "# Create time-based features\n",
    "df['hour_of_day'] = df['scheduled_time'].dt.hour\n",
    "df['day_of_week'] = df['scheduled_time'].dt.dayofweek\n",
    "df['month'] = df['scheduled_time'].dt.month\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "df['is_rush_hour'] = df['hour_of_day'].isin([7, 8, 9, 16, 17, 18, 19]).astype(int)\n",
    "df['is_peak_month'] = df['month'].isin([1, 7, 12]).astype(int)\n",
    "\n",
    "# Handle missing values\n",
    "df['delay_minutes'] = df['delay_minutes'].fillna(0)\n",
    "df['from_id'] = df['from_id'].fillna(-1)\n",
    "df['to_id'] = df['to_id'].fillna(-1)\n",
    "\n",
    "# Remove outliers\n",
    "q1 = df['delay_minutes'].quantile(0.25)\n",
    "q3 = df['delay_minutes'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "df = df[df['delay_minutes'].between(q1 - 1.5*iqr, q3 + 1.5*iqr)]\n",
    "\n",
    "# Create route frequency features\n",
    "route_freq = df.groupby(['from_id', 'to_id']).size().reset_index(name='route_frequency')\n",
    "df = df.merge(route_freq, on=['from_id', 'to_id'])\n",
    "\n",
    "# Create average delay by route\n",
    "route_avg_delay = df.groupby(['from_id', 'to_id'])['delay_minutes'].mean().reset_index(name='route_avg_delay')\n",
    "df = df.merge(route_avg_delay, on=['from_id', 'to_id'])\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "if 'line' in df.columns:\n",
    "    df = pd.get_dummies(df, columns=['line'], prefix='line')\n",
    "if 'type' in df.columns:\n",
    "    df = pd.get_dummies(df, columns=['type'], prefix='type')\n",
    "if 'status' in df.columns:\n",
    "    df = pd.get_dummies(df, columns=['status'], prefix='status')\n",
    "\n",
    "# Step 3: Prepare Features and Target\n",
    "print(\"Preparing features...\")\n",
    "features = [\n",
    "    'hour_of_day', 'day_of_week', 'month', \n",
    "    'is_weekend', 'is_rush_hour', 'is_peak_month',\n",
    "    'from_id', 'to_id', 'route_frequency', 'route_avg_delay'\n",
    "]\n",
    "\n",
    "# Add one-hot encoded columns\n",
    "features.extend([col for col in df.columns if col.startswith(('line_', 'type_', 'status_'))])\n",
    "\n",
    "# Verify all features exist in dataframe\n",
    "features = [f for f in features if f in df.columns]\n",
    "print(f\"Using features: {features}\")\n",
    "\n",
    "X = df[features]\n",
    "y = df['delay_minutes']\n",
    "\n",
    "# Step 4: Split the data\n",
    "print(\"Splitting data...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Initialize and train model\n",
    "print(\"Training model...\")\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Predictions and Evaluation\n",
    "print(\"Evaluating model...\")\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f} minutes\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f} minutes\")\n",
    "\n",
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# Create station mappings\n",
    "from_station_id = df[['from', 'from_id']].drop_duplicates().dropna()\n",
    "from_station_id = dict(zip(from_station_id['from'], from_station_id['from_id']))\n",
    "\n",
    "to_station_id = df[['to', 'to_id']].drop_duplicates().dropna()\n",
    "to_station_id = dict(zip(to_station_id['to'], to_station_id['to_id']))\n",
    "\n",
    "# Create route dictionaries\n",
    "route_freq_dict = dict(zip(zip(route_freq['from_id'], route_freq['to_id']), \n",
    "                          route_freq['route_frequency']))\n",
    "route_avg_dict = dict(zip(zip(route_avg_delay['from_id'], route_avg_delay['to_id']), \n",
    "                         route_avg_delay['route_avg_delay']))\n",
    "\n",
    "# Prediction functions\n",
    "def predict_delay(hour_of_day, day_of_week, from_id, to_id, month=None):\n",
    "    if month is None:\n",
    "        month = pd.Timestamp.now().month\n",
    "        \n",
    "    # Get route statistics\n",
    "    route_freq = route_freq_dict.get((from_id, to_id), 0)\n",
    "    route_avg = route_avg_dict.get((from_id, to_id), 0)\n",
    "    \n",
    "    # Create input data\n",
    "    input_data = pd.DataFrame([{\n",
    "        'hour_of_day': hour_of_day,\n",
    "        'day_of_week': day_of_week,\n",
    "        'month': month,\n",
    "        'is_weekend': 1 if day_of_week in [5, 6] else 0,\n",
    "        'is_rush_hour': 1 if hour_of_day in [7, 8, 9, 16, 17, 18, 19] else 0,\n",
    "        'is_peak_month': 1 if month in [1, 7, 12] else 0,\n",
    "        'from_id': from_id,\n",
    "        'to_id': to_id,\n",
    "        'route_frequency': route_freq,\n",
    "        'route_avg_delay': route_avg\n",
    "    }])\n",
    "    \n",
    "    # Add any missing columns\n",
    "    for col in features:\n",
    "        if col not in input_data.columns:\n",
    "            input_data[col] = 0\n",
    "    \n",
    "    # Ensure columns are in the same order as training\n",
    "    input_data = input_data[features]\n",
    "    \n",
    "    return model.predict(input_data)[0]\n",
    "\n",
    "def predict_delay_by_station_names(hour_of_day, day_of_week, from_station, to_station, month=None):\n",
    "    from_id = from_station_id.get(from_station)\n",
    "    to_id = to_station_id.get(to_station)\n",
    "    \n",
    "    if from_id is None or to_id is None:\n",
    "        return \"Invalid station name(s)\"\n",
    "        \n",
    "    return predict_delay(hour_of_day, day_of_week, from_id, to_id, month)\n",
    "\n",
    "\n",
    "\n",
    "# Example prediction\n",
    "print(\"\\nTesting prediction...\")\n",
    "try:\n",
    "    example_from = list(from_station_id.keys())[0]\n",
    "    example_to = list(to_station_id.keys())[0]\n",
    "    prediction = predict_delay_by_station_names(8, 1, example_from, example_to)\n",
    "    print(f\"Predicted delay for {example_from} to {example_to}: {prediction:.2f} minutes\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in example prediction: {e}\")\n",
    "\n",
    "print(\"\\nProcess completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing data...\n",
      "Preparing features...\n",
      "Using features: ['hour_of_day', 'day_of_week', 'month', 'is_weekend', 'is_rush_hour', 'from_id', 'to_id', 'line_Atl. City Line', 'line_Bergen Co. Line ', 'line_Gladstone Branch', 'line_Main Line', 'line_Montclair-Boonton', 'line_Morristown Line', 'line_No Jersey Coast', 'line_Northeast Corrdr', 'line_Pascack Valley', 'line_Princeton Shuttle', 'line_Raritan Valley', 'type_NJ Transit', 'status_cancelled', 'status_departed', 'status_estimated']\n",
      "Splitting data...\n",
      "Training model...\n",
      "Evaluating model...\n",
      "\n",
      "Model Performance:\n",
      "Mean Absolute Error (MAE): 1.40 minutes\n",
      "Root Mean Squared Error (RMSE): 1.86 minutes\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                   feature  importance\n",
      "6                    to_id    0.194014\n",
      "0              hour_of_day    0.183709\n",
      "5                  from_id    0.179001\n",
      "20         status_departed    0.129046\n",
      "2                    month    0.076188\n",
      "7      line_Atl. City Line    0.038401\n",
      "1              day_of_week    0.029853\n",
      "4             is_rush_hour    0.026439\n",
      "11  line_Montclair-Boonton    0.023770\n",
      "15     line_Pascack Valley    0.015867\n",
      "\n",
      "Testing prediction...\n",
      "Predicted delay: 0.63 minutes\n",
      "\n",
      "Process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Step 1: Load Data\n",
    "print(\"Loading data...\")\n",
    "data_path = '/Users/chetan/Documents/GitHub/nj_transit_data_ru_hack/data/data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "print(\"Preprocessing data...\")\n",
    "\n",
    "# Convert datetime columns\n",
    "df['scheduled_time'] = pd.to_datetime(df['scheduled_time'], errors='coerce')\n",
    "df['actual_time'] = pd.to_datetime(df['actual_time'], errors='coerce')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Drop rows where time conversion failed\n",
    "df = df.dropna(subset=['scheduled_time', 'actual_time'])\n",
    "\n",
    "# Create basic features\n",
    "df['hour_of_day'] = df['scheduled_time'].dt.hour\n",
    "df['day_of_week'] = df['scheduled_time'].dt.dayofweek\n",
    "df['month'] = df['scheduled_time'].dt.month\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "df['is_rush_hour'] = df['hour_of_day'].isin([7, 8, 9, 16, 17, 18, 19]).astype(int)\n",
    "\n",
    "# Handle missing values\n",
    "df['delay_minutes'] = df['delay_minutes'].fillna(0)\n",
    "df['from_id'] = df['from_id'].fillna(-1)\n",
    "df['to_id'] = df['to_id'].fillna(-1)\n",
    "\n",
    "# Remove outliers\n",
    "q1 = df['delay_minutes'].quantile(0.25)\n",
    "q3 = df['delay_minutes'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "df = df[df['delay_minutes'].between(q1 - 1.5*iqr, q3 + 1.5*iqr)]\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "if 'line' in df.columns:\n",
    "    df = pd.get_dummies(df, columns=['line'], prefix='line')\n",
    "if 'type' in df.columns:\n",
    "    df = pd.get_dummies(df, columns=['type'], prefix='type')\n",
    "if 'status' in df.columns:\n",
    "    df = pd.get_dummies(df, columns=['status'], prefix='status')\n",
    "\n",
    "# Step 3: Prepare Features\n",
    "print(\"Preparing features...\")\n",
    "features = [\n",
    "    'hour_of_day', 'day_of_week', 'month', \n",
    "    'is_weekend', 'is_rush_hour',\n",
    "    'from_id', 'to_id'\n",
    "]\n",
    "\n",
    "# Add one-hot encoded columns\n",
    "features.extend([col for col in df.columns if col.startswith(('line_', 'type_', 'status_'))])\n",
    "\n",
    "# Verify all features exist\n",
    "features = [f for f in features if f in df.columns]\n",
    "print(f\"Using features: {features}\")\n",
    "\n",
    "X = df[features]\n",
    "y = df['delay_minutes']\n",
    "\n",
    "# Step 4: Split the data\n",
    "print(\"Splitting data...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train model\n",
    "print(\"Training model...\")\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate model\n",
    "print(\"Evaluating model...\")\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f} minutes\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f} minutes\")\n",
    "\n",
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# Simple prediction function\n",
    "def predict_delay(hour_of_day, day_of_week, from_id, to_id, month=None):\n",
    "    if month is None:\n",
    "        month = pd.Timestamp.now().month\n",
    "        \n",
    "    input_data = pd.DataFrame([{\n",
    "        'hour_of_day': hour_of_day,\n",
    "        'day_of_week': day_of_week,\n",
    "        'month': month,\n",
    "        'is_weekend': 1 if day_of_week in [5, 6] else 0,\n",
    "        'is_rush_hour': 1 if hour_of_day in [7, 8, 9, 16, 17, 18, 19] else 0,\n",
    "        'from_id': from_id,\n",
    "        'to_id': to_id\n",
    "    }])\n",
    "    \n",
    "    # Add dummy columns for categorical variables\n",
    "    for col in features:\n",
    "        if col not in input_data.columns:\n",
    "            input_data[col] = 0\n",
    "            \n",
    "    input_data = input_data[features]\n",
    "    return model.predict(input_data)[0]\n",
    "\n",
    "# Test prediction\n",
    "print(\"\\nTesting prediction...\")\n",
    "try:\n",
    "    example_prediction = predict_delay(8, 1, 105, 107)  # Example IDs\n",
    "    print(f\"Predicted delay: {example_prediction:.2f} minutes\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in example prediction: {e}\")\n",
    "\n",
    "print(\"\\nProcess completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /Users/chetan/Documents/GitHub/nj_transit_data_ru_hack/models/delay_prediction_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "model_dir = '/Users/chetan/Documents/GitHub/nj_transit_data_ru_hack/models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the trained model\n",
    "model_path = os.path.join(model_dir, 'delay_prediction_model1.joblib')\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...\n",
      "Model saved as: delay_model_20241027_051305.joblib\n"
     ]
    }
   ],
   "source": [
    "# Save model with timestamp and metrics\n",
    "print(\"\\nSaving model...\")\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_dir = '/Users/chetan/Documents/GitHub/nj_transit_data_ru_hack/models'\n",
    "model_name = f'delay_model_{timestamp}.joblib'\n",
    "\n",
    "joblib.dump(model, f'{model_dir}/{model_name}')\n",
    "print(f\"Model saved as: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
